<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [MG] Fwd: Re:  Democratizing Blockchain Governance in Versioning
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:start%40metagovernment.org?Subject=Re%3A%20%5BMG%5D%20Fwd%3A%20Re%3A%20%20Democratizing%20Blockchain%20Governance%20in%20Versioning&In-Reply-To=%3C5655faee-c457-f82b-f5f8-24e897895d2d%40abo.fi%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="017570.html">
   <LINK REL="Next"  HREF="017572.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[MG] Fwd: Re:  Democratizing Blockchain Governance in Versioning</H1>
    <B>Mikael Sand</B> 
    <A HREF="mailto:start%40metagovernment.org?Subject=Re%3A%20%5BMG%5D%20Fwd%3A%20Re%3A%20%20Democratizing%20Blockchain%20Governance%20in%20Versioning&In-Reply-To=%3C5655faee-c457-f82b-f5f8-24e897895d2d%40abo.fi%3E"
       TITLE="[MG] Fwd: Re:  Democratizing Blockchain Governance in Versioning">msand at abo.fi
       </A><BR>
    <I>Tue Apr 17 15:00:10 EDT 2018</I>
    <P><UL>
        <LI>Previous message (by thread): <A HREF="017570.html">[MG] Fwd: Re:  Democratizing Blockchain Governance in Versioning
</A></li>
        <LI>Next message (by thread): <A HREF="017572.html">[MG] Fwd: Re: Fwd: Re: Democratizing Blockchain Governance in Versioning
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html">[ date ]</a>
              <a href="thread.html">[ thread ]</a>
              <a href="subject.html">[ subject ]</a>
              <a href="author.html">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Managed to send it privately again. I need to pay more attention to 
where I'm clicking ;)



-------- Forwarded Message --------
Subject: 	Re: [MG] Democratizing Blockchain Governance in Versioning
Date: 	Tue, 17 Apr 2018 21:59:06 +0300
From: 	Mikael Sand &lt;<A HREF="http://metagovernment.org/mailman/listinfo/start_metagovernment.org">msand at abo.fi</A>&gt;
To: 	Scott Raney &lt;<A HREF="http://metagovernment.org/mailman/listinfo/start_metagovernment.org">metamerman at gmail.com</A>&gt;



Oh sorry, did intend it for the list. I'll resend it, and, I guess I can 
just reply to your new comments here as you asked to repost. You can 
repost your mail in between as well if you wish, but I'll try to keep 
your replies intact.


On 4/16/2018 11:42 PM, Scott Raney wrote:
&gt;<i> Did you intend to send this just to me, or to the metagovernment list
</I>&gt;<i> too?  Let me know, and I'll repost this message to that list if
</I>&gt;<i> needed...
</I>&gt;<i>
</I>&gt;<i> On Sun, Apr 15, 2018 at 12:09 PM, Mikael Sand&lt;<A HREF="http://metagovernment.org/mailman/listinfo/start_metagovernment.org">msand at abo.fi</A>&gt;  wrote:
</I>&gt;<i>
</I>&gt;<i> (big snip: sounds a little &quot;conspiracy theoretical&quot; to me...)
</I>I understand your sentiment, unfortunately the security faults are 
available in published research, I can dig up some peer-reviewed papers 
and videos from IT security conferences if you're interested, but some 
googling should suffice to find how to do it yourself with relative ease 
(often with source code, sometimes just the rough sketch of it). The 
chaos communication congress, black hat and def con conference materials 
should have you taking out your tinfoil hat quite fast if you're 
sensitive to conspiratorial speculation. But the technology exists, and 
just accounts for the consequences of the current physical 
implementations, hardware and protocol designs, and can mostly be 
verified simply by thinking from first principles and the 
specifications. I'm not theorizing about who might be conspiring to use 
this or not, for whatever reasons, I just either include these published 
facts in a specific IT security threat model, or not, depending on the 
use-case, for most IT systems they're completely irrelevant. The 
importance is not on if it is happening, but rather if it is 
theoretically possible at all, physically possible to do within a 
certain budget, and if the relevant potential actors have incentives to 
spend that budget on it.

If you work with IT security you need to take these publications into 
account in some of your threat models either way, they can of course 
have varying levels of paranoia in their assumptions, like if you 
include state actors and intelligence agencies as potential adversaries 
in them it completely changes the picture. Even the top level domain 
resolution of the dns system, certificate authorities and the signalling 
system 7 (used to set up and route connections, phone-calls and sms 
etc.) have known faults and lacks trust in this case. DNSSEC and 
DNSCrypt helps somewhat, but only keys shared and verified either in 
physical contact or over already secured communications channels and 
webs of trust has a chance of handling that as far as I know. And, one 
time padding if you need actual secrecy of course, but that won't scale 
before we have a cheap source of bell states on a global quantum internet.

Initial trust needs to happen between people who know each other and 
meet in real life, then a OpenPGP like web of trust can scale the 
network of public keys used for signing the messages (and encrypting if 
you need secrecy). Duniter has the most interesting attempt I've seen so 
far, for building a web-of-trust and handling the identities and 
accounting of who is still living/interacting with the economy in this 
manner. The six signatures within the last 100 days and max-distance of 
six might not be perfect, but have to start experimenting and measuring 
the results somewhere.
&gt;&gt;<i> Well, having a centralized service being capable of scaling out and handling
</I>&gt;&gt;<i> DDOS is one thing. Having it truly p2p and decentralized without any single
</I>&gt;&gt;<i> points of failure is another, further, having it work when any kind of
</I>&gt;&gt;<i> network is available is yet another (dat/beakerbrowser might be one of the
</I>&gt;&gt;<i> easiest ways to share files cross-platform on a lan to this day).
</I>&gt;<i> The issue of network reliability is really orthogonal to the issue of
</I>&gt;<i> governance (i.e., we've become dependent on a functioning Internet for
</I>&gt;<i> far more than just the ability to run the government).
</I>Well, if you're going to scale the system out and make it distributed to 
be able to handle large loads, network partitions and ddos attacks, then 
you'll end up either implementing or using a consensus algorithm of some 
sort. Are you familiar with the 
<A HREF="https://en.wikipedia.org/wiki/CAP_theorem">https://en.wikipedia.org/wiki/CAP_theorem</A> ? In short, you can only have 
two out of these three: consistency, availability and partition 
tolerance. You mentioned mongodb before, which now supports running a 
primary (+ secondary replicas) and providing either BASE (Basically 
Available, Soft state, Eventual consistency) semantics or, starting this 
summer in v4.0, multi table ACID (Atomicity, Consistency, Isolation, 
Durability) semantics. What conflict resolution strategy are you 
suggesting for network partitions? Lets assume AWS or a large part of it 
goes down for some amount of time, is the service unavailable until the 
connections recover? And the service would depend on a functioning 
connection to wider Internet? And thus wouldn't work for organizing 
people if e.g. the government, a coup, or a foreign military shuts down 
the telecommunications infrastructure?
&gt;&gt;<i> Not trusting anyone seems like a fools game, but at least I don't trust
</I>&gt;&gt;<i> everyone, almost always someone can perceive incentives to exploit for
</I>&gt;&gt;<i> economic reasons or otherwise.
</I>&gt;<i> Agreed: And if there's going to be one person you *have* to trust
</I>&gt;<i> (outside your close friends and family, of course), it's got to be the
</I>&gt;<i> manager of your local government. Without that, I submit that there
</I>&gt;<i> *is* no functioning local government.
</I>&gt;<i>
</I>&gt;<i> (snip, more conspiracy theory stuff)
</I>&gt;<i>
</I>&gt;&gt;<i> Are you suggesting people do it
</I>&gt;&gt;<i> manually by hand when they for whatever reason feel the urge to check if
</I>&gt;&gt;<i> their data has been manipulated? And using what process? Opening the website
</I>&gt;&gt;<i> and checking their profile? How does this say anything about how that data
</I>&gt;&gt;<i> has been counted or how it relates to other peoples data?
</I>&gt;<i> For example, in proxyfor.me you can download the complete data file
</I>&gt;<i> for the vote on any proposal. You can check your vote along with
</I>&gt;<i> everyone else's as well as ensuring that everything adds up correctly.
</I>&gt;<i> And anyone who can use a spreadsheet (or even a text editor) can do
</I>&gt;<i> this. Nothing fancy required, no trust required other than that the
</I>&gt;<i> Local Manager has verified that the screen names correspond to real
</I>&gt;<i> people. And again, if you don't trust at least that executive, there
</I>&gt;<i> *is* no functioning government so this is simply not a problem. And
</I>&gt;<i> there is even a rough check you can do to detect obvious tampering:
</I>&gt;<i> Voting rolls are public so we already *know* how many votes there
</I>&gt;<i> should be...
</I>So, lets assume you've downloaded the archive once, and do it again to 
check if any of the old data has changed, and you notice a chunk of it 
missing or modified? What now? How do we find the culprit? Was it the 
manager? Some IT admin? A bug? An attack?
How much traffic can it handle for this?

Lets say, a majority of users (or relatively large number) would decide 
to download the entire archive once or more per day, is this cost 
effective? and simple? Perhaps from some perspective, but I don't see 
how the auditing would be done reliably without making the normal 
verifying users essentially like a ddos attack once the systems gets 
large amounts of users. And besides, downloading and checking the data 
would only detect the issue, not say what caused it, nor resolve it 
automatically using an algorithm made for distributed systems.

Lets say each user needs to generate a private and public key, and sign 
their votes/actions/data whenever they add/change something in the 
service and include a reference to what was the latest version of the 
state, persist this in an event store, and calculate a checksum from the 
checksum in the previous last event and the contents of the entire new 
event (like git). Then we know it was someone in possession of the 
private key corresponding to the public key of the user who 
created/caused the change. At this stage you would already have what 
amounts to a directed acyclic graph. Can very well be stored in mongodb, 
or essentially any other persistence layer. Then if you just add a 
consensus algorithm (based e.g. on vector clocks, matrix clocks, 
interval tree clocks, or general causal trees), you can make it into a 
distributed system which can handle availability, and using something 
like latest vote/write wins you can handle conflicts on a per 
user/private key event log basis to get eventual consistency (and using 
CRDT and/or OT for real-time collaborative data), thus working in p2p 
fashion in any network conditions (even highly unreliable and 
intermittent ones).

Hmm, you intend a local manager to verify the identity of all the people 
using the system? This seems like quite another bottleneck. What would 
be the process for verifying the identity? Is the local manager the only 
one who knows what screen name corresponds to what living person? Or 
what's the auditing process here? What happens when we notice our dead 
neighbor adding new votes to the data a few months after them passing away?
&gt;&gt;<i> At least, each electronic voting system that has been built by/for the
</I>&gt;&gt;<i> finnish government has been bug ridden and full of insane security problems,
</I>&gt;&gt;<i> while costing hundreds of millions of euros. I'm not sure what technological
</I>&gt;&gt;<i> simplifications you're suggesting to achieve reliable and secure software
</I>&gt;&gt;<i> engineering by/for governments.
</I>&gt;<i> Well getting rid of the ridiculous &quot;secret ballot&quot; requirement is a
</I>&gt;<i> big one. It's not necessary and trying to impose it just makes the
</I>&gt;<i> system non-verifiable. Which perhaps not coincidentally is exactly the
</I>&gt;<i> same problem any blockchain-based system has: If the average person
</I>&gt;<i> can't go in and validate the vote count, IMHO the system can never be
</I>&gt;<i> trusted.
</I>Secret ballots in paper-less electronic voting are inherently 
incompatible with verify-ability of either the tally or one person one 
vote. It makes sense in paper-trail voting, which is required for any 
vote-buying/coercion sensitive topics and decisions. But as far as I 
understand now, any kind of public internet voting is only suitable for 
completely open data. I didn't actually mention secret ballots so far, 
and I'm not sure why you're bringing it up.
A private group (already knowing each others public keys) can create 
issue specific keys shared within the group, and use symmetric 
cryptography to vote in secret from the public on a public ledger, while 
maintaining immutability and the potential to audit the decision history 
later on. But, this is more relevant to e.g. a security conscious boards 
of directors or some specific interests groups, and will probably be 
kept in private &quot;block-chains&quot; or using linked timestamping anyway, 
which is nothing new. Calculating signatures and checksums for data 
integrity checking has been implemented many times over in e.g. all 
kinds of military and banking databases, bank-to-bank communications and 
others considering similar threat vectors etc. long before bitcoin came. 
Inter-bank comms tend to use some of the best key distribution 
mechanisms money can buy, then again, many consumer facing internet 
banking apps / checkout flows have mostly crap security and very basic 
flaws like no escape character handling together with user modifiable 
content and hashes, these fall under the &quot;don't give a shit&quot; policy of 
banks, insurance covers it, and mostly a consumer client risk factor. 
And, block-chain is mostly just a slightly catchier word that happened 
to reach the mainstream because of the popularity and hype around 
bitcoin and other crypto-currencies.
&gt;&gt;<i> Unless it's based on some
</I>&gt;&gt;<i> correct-by-construction software design or otherwise proof-based on some
</I>&gt;&gt;<i> sound type theory, I'm not sure it'll be sufficient, and that is certainly
</I>&gt;&gt;<i> not a simplification, it easily makes development time 10x or more (I have a
</I>&gt;&gt;<i> fair share of experience from trying myself, while I was studying for my
</I>&gt;&gt;<i> computer science masters degree).
</I>&gt;<i> All software has bugs. The goal is not to produce perfect code, just
</I>&gt;<i> good enough code such that it does the job *and* having a proper
</I>&gt;<i> design such that you can at least *detect* when a bug/hack/loss has
</I>&gt;<i> occurred. Finland's system, along with all blockchain-based systems,
</I>&gt;<i> fail to deliver the latter, whereas proxyfor.me delivers it easily.
</I>I'm still confused as to what the auditing process would be in 
proxyfor.me? What is the conflict resolution method once someone claims 
they have an older backup including data which is missing and/or some 
which is fabricated in the current db? What if there are more than one 
actor claiming this? With mutually conflicting claims?
How much experience in software development and running distributed 
system in production do you have? I'm very interested in seeing your 
design and how it easily verifies the good enough correctness, and its 
ability to detect bugs, hacks and losses. I'm quite skeptical you've 
done any of this yet though, without actually recreating what the field 
of computer scientists and cryptographers have made for these specific 
purposes. Perhaps there might be some technical jargon I've used which 
deserves clarification, please ask if I've used some terms too 
ambiguously or in an unclear way, I'm doing my best to express this as 
comprehensibly as I can, while I completely understand if it seems very 
confusing.
&gt;&gt;<i> I sure trust it to mostly
</I>&gt;&gt;<i> behave according to the current laws governing money and accounting, as in
</I>&gt;&gt;<i> the sources and sinks of debt, interest rates and fractional reserve
</I>&gt;&gt;<i> currency; and the mathematical consequences of those.
</I>&gt;<i> And that's all I'm assuming you should have to do. Or do with the
</I>&gt;<i> voting system (i.e., you should have the source code and all the raw
</I>&gt;<i> input and output data and so can verify its operation).
</I>&gt;<i>
</I>&gt;<i> I'm certainly not expecting anyone to trust the *political* components
</I>&gt;<i> of monetary policy, which I agree with you is as screwed up as the
</I>&gt;<i> rest of government (misrepresentative democracy).
</I>Hmm, well the source code and a sample of the dataset is not really 
enough to ensure the correct functioning of a distributed system. Among 
other things, it doesn't tell if its refusing to respond to some users, 
or if it only shows what the person attempting to verify the data wants 
to see about their own data, they would still have to check with others 
to see if what they see as others data is actually the same as what the 
others see, and establish some common knowledge, about what they know, 
what others know, who knows that who knows what transitively, and so on. 
I'd much rather have the algorithm establish this rather than do it 
manually. If the specification is a protocol, you don't have to trust 
the specific implementations, as long as the protocol constrains the 
possible interactions correctly, you can use any specification compliant 
client to interact with the network and verify what parts of the data 
has already reached consensus in what parts of the network.
&gt;&gt;<i> Do you have any technical objections to this? Or does this boil down to you
</I>&gt;&gt;<i> wanting the executive branch to be trusted to run a software in a
</I>&gt;&gt;<i> centralized fashion? For what reasons/benefits? To be &quot;vastly simplified,
</I>&gt;&gt;<i> improving service levels, reliability, and the ability to detect and deal
</I>&gt;&gt;<i> with attacks on the system&quot;?
</I>&gt;<i> Exactly.
</I>&gt;<i>
</I>&gt;&gt;<i> What becomes simpler for who? What service level/reliability increases?
</I>&gt;<i> Simpler to understand, simpler implement, simpler to maintain, even in
</I>&gt;<i> the face of attach (AWS, for example, has *vastly* more capability in
</I>&gt;<i> dealing with this kind of thing than *any* government has, or could
</I>&gt;<i> ever hope to have).
</I>Too simple in fact, at least to be capable of working as a verifiable 
distributed system; unless you intend to have a single primary, in which 
case you don't have an actual distributed system with availability, just 
some extra scaling out for more and faster reads when network conditions 
are good; or unless you use something like Lamport timestamps/Vector 
clocks/Matrix clocks/Version vectors/Interval Tree Clocks etc to record 
the partial ordering of events and capturing the chronological and 
causal relationship in a distributed manner; and, verification of 
authenticity, unless you add signatures; and, verification of integrity, 
unless you add something like merkle trees. What parts of the process 
confuses you? Or what part is not simple enough? Or what parts of these 
do you think you're achieving without doing the necessary work? 
Essentially a relatively simple interface can describe the entire api 
surface needed in a thin-client, the signature and checksum algorithm 
choices have acceptable reasoning behind them, otherwise it's a p2p 
distributed database like almost any other, except that it actually 
works in an existing browser with self-hosting and authoring capability, 
like Tim Berners-Lee intended the web to be.
&gt;&gt;<i> How do external users detect a virus or
</I>&gt;&gt;<i> backdoor in the actual running system? How does anyone verify what source
</I>&gt;&gt;<i> code is used?
</I>&gt;<i> You can't, but that's true of blockchain or any other distributed
</I>&gt;<i> system. Who do you trust more, one executive that *we* voted into
</I>&gt;<i> office and can check up on or dozens or even hundreds of unknown
</I>&gt;<i> servers run by who knows who?
</I>Well, with a DAG, signature and checksum checking, you can use a 
consensus algorithm to agree on what data should exist and it doesn't 
matter as long as less than the majority of the network is infected, and 
a web-of-trust based system can even handle that to a large degree. I 
certainly trust cryptography and the very small likelihood that someone 
would posses/deduce/brute-force all the private keys, much more, than 
any centralized IT-system without either the required data nor 
algorithms used for sufficient integrity checking and conflict resolution.
&gt;<i>   
</I>&gt;&gt;<i> That the compiler and virtual machines are working correctly?
</I>&gt;&gt;<i> OS? Spectre variant 2 patches applied? What else is running on the same
</I>&gt;&gt;<i> hardware and network or has physical access to it? Other side-channel
</I>&gt;&gt;<i> attacks? How would the centralized admins themselves even do these things?
</I>&gt;<i> Again, it's got to be open voting so all this secrecy stuff just goes
</I>&gt;<i> away (who cares if another process on the same CPU can read the voting
</I>&gt;<i> systems' memory? It's all open and public anyway!)
</I>Well, it's not about someone being able to read it, everybody being able 
to read it is actually a requirement rather than something to avoid, a 
DAG would also have all the data openly readable (unless you write some 
encrypted data instead of actual text in a comment field or something, 
or if the protocol allows storing arbitrary data then anything can be 
stored of course), and anyone wanting to keep their local copy of the 
database up-to-date is able to, and can vote while off-line/grid and 
distribute the results once they have a connection again, can even have 
actual sneakernet as courier of votes/data to/from remote places.
But rather, it's about the potential for backdoors and user specific 
massaging of what it responds with, and what is actually used in other 
calculations/responses. Simply put, the users need to be able to reach 
consensus about the data in the service, otherwise data can 
disappear/change without a trace of who/what caused it or any way to 
resolve the conflict, thus deserving criticism and eroding trust in the 
system before it even gets started.
&gt;&gt;<i> It seems to me it opens up several classes of vulnerabilities. I'm not sure
</I>&gt;&gt;<i> what your threat model and security analysis method is here. But I don't see
</I>&gt;&gt;<i> anything of substance to back up these claimed benefits.
</I>&gt;<i> Only if you think like a secrecy-obsessed paranoid. If it's all open
</I>&gt;<i> nearly all of your reservations simply disappear, and the rest can
</I>&gt;<i> easily be dealt with by using the best commercially-available services
</I>&gt;<i> rather than trusting JimBob's basement server or those run by the
</I>&gt;<i> Russians like the blockchain proponents would propose.
</I>&gt;<i>    Regards,
</I>&gt;<i>      Scott
</I>Well, you might have confused me for someone else, I haven't advocated 
for secrecy in any public internet voting system or any other kind of 
e-democracy platform, (perhaps many years ago for privacy sensitive 
topics, but not since actually looking into building them and reading 
some of the security analysis around them). The p2p web is essentially 
the opposite to secrecy. And my concerns are mostly about data integrity 
and authenticity, to make it good enough such that the security minded 
people would be happier with proxyfor.me and to make it feasible at all 
to audit what is going on. I'm not sure how you could have confused any 
of my reservations being related to secrecy. The only relation to or 
concern about secrecy is perhaps of someone in the executive branch or 
an external malicious actor attempting to delete/change/add data in secret.

I'm not sure what you consider &quot;the best commercially-available&quot;, but 
the <A HREF="https://en.wikipedia.org/wiki/Merkle_tree">https://en.wikipedia.org/wiki/Merkle_tree</A> seems almost universally 
common. Not sure what brand of it you would consider simple enough. But 
if the best commercial options suffice, it seems it's ok to use it as 
long as we don't call it a blockchain, or what counts as simple in 
distributed systems really? ;) I think you might have confused what I'm 
suggesting with nakamoto consensus, which I don't think is a good fit 
for this. Dat, ipfs and (secure scuttlebutt) ssb are perhaps the most 
evolved forms of merkle trees for the p2p web, even firefox allows 
supporting <A HREF="dat://">dat://</A> <A HREF="ipfs://">ipfs://</A> and <A HREF="ssb://">ssb://</A> urls now: 
<A HREF="https://blog.mozilla.org/addons/2018/01/26/extensions-firefox-59/">https://blog.mozilla.org/addons/2018/01/26/extensions-firefox-59/</A> and 
DNS integration will keep getting better.

Consensus and distributed systems sure ain't very simple to reason about 
that's for sure. But putting an arbitrary limit somewhere such that it's 
not even possible to at least eventually resolve conflicts, or that the 
system is unavailable in network partitions doesn't seem very great. 
These things need to be dealt with on the protocol level, instead of 
being hopeful that some lazy it-admins are top notch and keeping 
everything in shape in a centralized system (especially in government 
IT, where the contracts tend to go to the same old friends each time, at 
least in finland), when in reality they tend to be ignorant of many 
security issues and they don't even admit/know about it/think it's an issue.

If you think that the dat project and the beaker browsers have bad 
choices in their technological decisions compared to some commercial 
alternative, I'm very interested in finding out about it and improving 
the p2p web using it. But I think you might find that the commercially 
available service implement these same categories of general algorithms, 
and in many cases the same specific choices, to solve the issues I'm 
trying to highlight here.

Sorry for the essay length ramblings, hope at least someone finds some 
of it useful.

Regards,
Mikael
-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;<A HREF="http://metagovernment.org/pipermail/start_metagovernment.org/attachments/20180417/3f2b3919/attachment-0001.html">http://metagovernment.org/pipermail/start_metagovernment.org/attachments/20180417/3f2b3919/attachment-0001.html</A>&gt;
</PRE>






<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message (by thread): <A HREF="017570.html">[MG] Fwd: Re:  Democratizing Blockchain Governance in Versioning
</A></li>
	<LI>Next message (by thread): <A HREF="017572.html">[MG] Fwd: Re: Fwd: Re: Democratizing Blockchain Governance in Versioning
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html">[ date ]</a>
              <a href="thread.html">[ thread ]</a>
              <a href="subject.html">[ subject ]</a>
              <a href="author.html">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://metagovernment.org/mailman/listinfo/start_metagovernment.org">More information about the Start
mailing list</a><br>
</body></html>
